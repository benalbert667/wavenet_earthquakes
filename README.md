Ben Albert, Fred Mendoza  
COSI 149B Final Project  
May 2, 2019

# LANL Earthquake Prediction

For this assignment, our task was to build and train a model that is able to predict the time in seconds until the next
earthquake from a long signal of seismic data. Once the model is trained,the model should be used to predict a final
time for each `seg_id` (small sub-regions of the laboratory setup) until the next earthquake at that segment.

## Data

The data in `train.csv` (see https://www.kaggle.com/c/LANL-Earthquake-Prediction/data) consists of almost 630 million
time steps of seismic data (16-bit integers) collected from an experimental setup in a lab, each paired with a
`time_to_failure` in seconds until the next laboratory earthquake. There are 16 total earthquakes present in the data
with an average of ~11 seconds (~3 million time steps) between them.

Each `seg_id` is composed of 150 thousand time steps of the same form of `acoustic_data` produced by a subsection of the
whole lab setup.

## Model

When deciding what model to use we thought about the biggest issues at hand with the problem space and which models have
performed well on similar sets of data. Due to the vast nature of the data library presented we decided to go with a 
WaveNetas it has done state of the art generation of speech frequencies, which we also related back to our movement data.
In other words the reason that we decided to go with WaveNet was its ability to handle such large pieces of information.
When you break things like images and voice you must break each second down into a huge set of different data points.
WaveNet has made great leaps in the ability of processing over such clusters of data to produce generated images and
audio. What WaveNet demonstrates is an ability to be emulate one dimensional inputs in a way that would represent their
nature in the form of another frequency. We metaphorically saw WaveNet as training a model to be able to speak the
language of its particular input. We also saw WaveNet to be particularly well fitted for this particular environment of
study. As we were looking at feedback from other machines, we can use the features used on audio to generate different
speaking voices for interesting purposes. Mainly that we can allow for systematic distinguishers between different types
of earthquake simulators. Overall we found WaveNet to be an interesting answer to this question and thus we began to
develop a WaveNet model fitted to our needs.

### Implementations

We carried out two iterations of the WaveNet model for predicting the `time_to_failure` for each `seg_id` in the test
data.  While each implementation uses WaveNet as it's primary prediction model, they employ the network in different
ways.  Our primary goal for each iteration was to consider the context (shape, features, noise, etc.) of the data when
applying the WaveNet, which resulted in the two following implementations.

#### Iteration 1

The first implementation that we produced worked with TensorFlowâ€™s built-in WaveNet implementation.
This model had a limited use-case, designed to fit to a .WAV file of raw audio.
Since our data took the form of two columns packaged as a CSV file, we first separated these into two .WAV files to be
read by the network. Since seismic data and raw audio are both highly time-dense signals, we thought of WaveNet as a way
of simply predicting future time steps of `acoustic_data` the same way it generates audio. After generation, our
conception was to predict future time steps of `acoustic_data` until the WaveNet produced the pattern of an earthquake.
This would allow us to compute a prediction of the final `time_to_failure` by simply taking the number of seconds
(a multiple of the number of time steps) until the WaveNet's first prediction of an earthquake.

Our implementation ran into a wall when considering how exactly to quantify an "earthquake" generated by the WaveNet.
Though we can predict future data from previous time steps, the realization of an earthquake in the data varied greatly
across both `seg_id`s (more or less powerful earthquakes for each), and between each earthquake within `train.csv`.
This made the problem of classifying when the WaveNet generated a future earthquake equivalent to the original
problem, and yielded too much error for any meaningful predictions.

#### Iteration 2

Our second iteration, built upon Iteration 1's shortcoming involving a heuristic analysis of future predicted data, was
and end-to-end WaveNet model for directly predicting `time_to_failure` from convolutions on the `acoustic_data`.  Rather
than predicting the x_t+1 time step of `acoustic_data`, the model would be trained to predict y_t, the `time_to_failure`
at the last time step of the casual convolution.

Similarly to the raw audio WaveNet implementation from the original paper, this implementation should produce a Softmax
distribution predicting the `time_to_failure` waveform.

## Training, Performance, and Future Work

See `Wavenet.ipynb` for our training and submission code.

We implemented our Iteration 2 conception in a similar fashion to that of the raw audio WaveNet.  Since WaveNet
takes in and outputs categorical representations of the wave, we first applied a miu-law to the `acoustic_data`,
generating a non-linear quantization to be fed into the network.  Though this method was originally used for audio,
we found that the transformation contextualized the data much better than a linear quantization, which yielded only
a few different categories due to the disparity in seismic activity between an earthquake and general data.  When
reproducing `time_to_failure` however, we decided on the linear quantization because of the constant slope of the data.
This combination yielded the most diversity in the categorical form, minimizing the loss of information.

Our final model used 10 stacked layers of dilated casual convolutions, giving us a receptive field of 2^10 = 1024.  We
used an Adam optimizer with decreasing learning rates (over the 40 total hours of training, we completed a full epoch
with `learning_rate=1e-5`, and two more reduced size epochs with `learning_rate=1e-5`).  To batch the data after
quantization, we first split the data into "objects" - sections of length 2280 - then grouped them into batches of 112
objects each (giving us ~2500 total batches over the data).  Though these numbers do not perfectly divide all the data,
we found that this configuration minimized training time while also maximizing the data input between each optimizer
update.  Since our objects are randomly shuffled between each epoch, we considered the loss of a single object of only
2280 time steps negligible.

Though the paper introduces some ideas of training the model in parallel, our PyTorch WaveNet implementation was bounded
by PyTorch's learning paradigm.  Because of the model's sheer size, this project required us using an external computing
framework (we used Google Cloud Platform (GCP) with 8 CPUs at 52GB RAM).  Due to these restrictions, our model did not
fit the data as perfectly as we had hoped after 40 hours of training.  For future work on this model, we would advise
both making use of some more powerful computing tools (multiple GPUs), as well as the tuning of some model parameters
(resetting the dilation factor every certain number of layers, larger receptive field, smaller learning rate, etc.)
to help the model fit the data more accurately and quickly.

## Individual Contribution

Most work on this project was done evenly and in conjunction between both team members.  We found the bulk of the work
to be the iterative learning process of data manipulation, model understanding, and implementation design as we
attempted this complex implementation.  Smaller individual work was split up as follows:

#### Ben 

- Designing the second implementation of the WaveNet
- Training the second WaveNet
- Testing the second WaveNet

#### Fred 

- Implementing the first implementation of the WaveNet
- Helping to develop WaveNet second implementation once we saw that this implementation was better 
- Model Adjustments